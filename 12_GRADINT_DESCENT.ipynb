{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('insurance_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data[['age', 'affordibility']], new_data[['bought_insurance']],\n",
    "                                                    test_size=0.2, random_state=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_div_scaled = X_train.copy()\n",
    "X_train_div_scaled['age'] /= 100\n",
    "\n",
    "X_test_div_scaled = X_test.copy()\n",
    "X_test_div_scaled['age'] /= 100\n",
    "\n",
    "X_train_div_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_standard_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_standard_scaled = StandardScaler().fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train_min_max_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "X_test_min_max_scaled = MinMaxScaler().fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1 = keras.Sequential(\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        input_shape=(2,),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='ones',\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    ")\n",
    "\n",
    "model1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model1.fit(X_train, y_train, epochs=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2 = keras.Sequential(\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        input_shape=(2,),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='ones',\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    ")\n",
    "\n",
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model2.fit(X_train_div_scaled, y_train, epochs=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3 = keras.Sequential(\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        input_shape=(2,),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='ones',\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    ")\n",
    "\n",
    "model3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model3.fit(X_train_standard_scaled, y_train, epochs=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model4 = keras.Sequential(\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        input_shape=(2,),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='ones',\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    ")\n",
    "\n",
    "model4.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model4.fit(X_train_min_max_scaled, y_train, epochs=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.evaluate(X_test_div_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3.evaluate(X_test_standard_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model4.evaluate(X_test_min_max_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return (1 / (1 + np.exp(-y)))\n",
    "\n",
    "\n",
    "sigmoid(18)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coef, intercp = model3.get_weights()\n",
    "coef, intercp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "def prediction(age, afford):\n",
    "    y = coef[0] * age + coef[1] * afford + intercp\n",
    "    return sigmoid(y)\n",
    "\n",
    "prediction(0.47, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_loss(y, yhat):\n",
    "\n",
    "    ep = 1e-16\n",
    "    yhat = [max(i,ep) for i in yhat]\n",
    "    yhat = [min(i,1-ep) for i in yhat]\n",
    "    yhat = np.array(yhat)\n",
    "\n",
    "    return -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_descent(x1 , x2 , y , epochs , loss_threshold):\n",
    "    w1 = w2 = 1\n",
    "    b = 0\n",
    "    learning_function = 0.001\n",
    "    n = len(x1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        weighted_sum = x1 * w1 + x2 * w2 + b\n",
    "        yhat = sigmoid(weighted_sum)\n",
    "\n",
    "        loss = log_loss(y , yhat)\n",
    "\n",
    "        w1d = (1/n)*np.mean((yhat - y) * np.transpose(x1))\n",
    "        w2d = (1/n)*np.mean((yhat - y) * np.transpose(x2))\n",
    "\n",
    "        b = np.mean(yhat - y)\n",
    "\n",
    "        w1 = w1 - w1d * learning_function\n",
    "        w2 = w2 - w2d * learning_function\n",
    "\n",
    "        print(f'Epoch: {i}, Weight 1 (w1): {w1}, Weight 2 (w2): {w2}, Bias: {b}, Loss: {loss}')\n",
    "\n",
    "        if loss < loss_threshold:\n",
    "            break\n",
    "\n",
    "    return w1 , w2 , b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gradient_descent(X_train_standard_scaled[:,0] , X_train_standard_scaled[:,1] , y_train , 1000 , 0.4631)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}